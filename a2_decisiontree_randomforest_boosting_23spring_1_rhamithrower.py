# -*- coding: utf-8 -*-
"""A2_DecisionTree_RandomForest_Boosting_23Spring-1_RhamiThrower.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yQhNz7K7GuZk4zItZDT-wh-OYTHIc8At

# This jupyter notebook is prepared by Rhami Thrower.

# 1. Load Data and perform basic EDA (4pts total)

### 1.1 import libraries: numpy, pandas, matplotlib.pyplot, seaborn, sklearn (1pt)
"""

import numpy as np,pandas as pd, matplotlib.pyplot as plt, seaborn as sns, sklearn

"""### 1.2 Upload the dataset to your Google Drive, then using the following code, import the data to a pandas dataframe and show the count of rows and columns (0.5pt)"""

from google.colab import drive
drive.mount('/content/drive')
file_name = '/content/drive/MyDrive/Colab Notebooks/hr_data_.csv' #you may need to change this line depending on the location of your file in Google Drive
#with open(file_name, 'r') as file:
    # TODO

df = pd.read_csv(file_name)

"""### 1.3 Show the top 7 and bottom 7 rows (0.5pt)"""

# TODO
print(df.head(n = 7),'\n',df.tail(n=7))

"""### 1.4 Show if any column has null values (0.5pt)"""

# TODO is their any marker that indicates a missing column
names = list(df)
for col in names:
  if df[col].isna().any():
    print(col)

"""### 1.5 Show/Plot the count of unique target labels and discuss its imbalances and possible issues in using it for classification. (1.5pt)"""

#TODO
names = list(df)

for col in names:
  #get unique target labels
  print(df[col].unique())
  plt.hist(df[col])
  plt.title(col)
  plt.xticks(rotation = -90)
  plt.show()

"""For the features that are imbalanced is the city and city index along side relevent experince and gender and major discipline. for possiable issues that would be the target features that might result in issues are target, major discipline, and relevent experience. this is primarly because these features seem to be to vage for the data it represents.

# 2. Feature Selection and Pre-processing (25 pts total)

## 2.1 Preprocessing City (1+1+1+1 = 4pts total)

### 2.1.1 Plot no. of records per city so that the highest city counts are shown in descending order (1pt)
"""

countCity = {}
for i,r in df['city'].iteritems():
  if r in countCity:
    countCity[r] = countCity[r] + 1
  else:
    countCity[r] = 1
countCity = dict(sorted(countCity.items(), key=lambda item: item[1], reverse = True))
plt.bar(countCity.keys(),countCity.values())

"""### 2.1.2 How many rows belong to the count-wise top 4 cities in total and how many for the remaining? (1pt)"""

# TODO
count = 0
numRows = 0
replace = list()
for keys in countCity.keys():
  if count == 4:
    replace.append(keys)
  else:
    numRows = numRows + countCity[keys]
    count = count + 1
print("top 4 = ",numRows,'\nothers = ', 8956 - numRows)
print(replace)

"""### 2.1.3 Replace the city name with city_others if the city name is not among the top 4 (1pt)"""

# TODO
others_df = df.replace(replace,'city_others')

"""### 2.1.4 Show some sample data that the records have changed correctly. (1pt)"""

# TODO
plt.hist(others_df['city'])

"""## 2.2. Preprocessing Education Level (1+2+2+1 = 6pts total)

### 2.2.1. Show the unique values of education level. (1pt)
"""

# TODO
educationLvl = df['education_level'].unique()
print(educationLvl)

"""### 2.2.2. Write a function named replace_labels() that can replace labels using given {old_label:new_label} dictionary (2pts)

Parameters: (1) dataframe, (2) a column name, (3) a dictionary with {old_label:new_label} mapping.

Returns: a dataframe with specified column values replaced with the  

"""

# TODO
def replace_labels(dataFrame, colName, replace_dictionary):
  dframe = dataFrame[colName].replace(replace_dictionary.keys(), replace_dictionary.values())
  return dframe

"""### 2.2.3. Using the replace_labels() function you just created, replace education_level column with ordinal values. The mapping can be like "Graduate":0, "Masters":1, "Phd":2 . (2pt)"""

# TODO
r_dictionary = {"Graduate":0, "Masters":1, "Phd":2}
new_edl = replace_labels(df,"education_level", r_dictionary)
df['education_level'] = new_edl

"""### 2.2.4 Show some sample data that the records have changed appropriately (1pt)"""

# TODO
print(df['education_level'].unique())

"""## 2.3. Preprocessing company_size (2+2+1 = 5pts total)

### 2.3.1 Show the unique values of the company_size column and their counts (2pt)
"""

companydict = {}
comp_size_vals = df['company_size'].unique();
for vals in comp_size_vals:
  companydict[vals] = 0
for rows in df['company_size']:
  companydict[rows] = companydict[rows] + 1
for k in companydict.keys():
  print(k, ' : ' , companydict[k])

"""### 2.3.2 Change the values of the company_size column from 0 to 7 where e0 is <10 and 7 is 10000+. The order of the numbers should be based on the values of the column-like an ordinary variable. (2pt)
(Hint: you can use the replace_labels() function you created before.)
"""

# TODO
change = {'<10': 0,'10/49': 1,'50-99': 2,'100-500': 3,'500-999': 4,'1000-4999': 5,'5000-9999': 6,'10000+': 7}
compSizeReplace = replace_labels(df,'company_size',change)
df['company_size'] = compSizeReplace

"""### 2.3.3 Show the updated unique values to validate they changed appropriately (1pt)"""

# TODO
print(df['company_size'].unique())

"""## 2.4. Preprocessing last_new_job (1+2+1 = 4pts total)

### 2.4.1 Show unique values of the last_new_job column (1pt)
"""

# TODO
lastNewDict = {}
lastnew_size_vals = df['last_new_job'].unique();
for vals in lastnew_size_vals:
  lastNewDict[vals] = 0
for rows in df['last_new_job']:
  lastNewDict[rows] = lastNewDict[rows] + 1
for k in lastNewDict.keys():
  print(k)

"""### 2.4.2 Convert the values of this column to never->0, 1->1,....>4 -->5 (2pt)
Hint: replace_labels()
"""

# TODO
change = {'never':0, '1': 1, '2': 2,'3':3, '4':4,'>4':5}
df['last_new_job'] = replace_labels(df,'last_new_job',change)

"""### 2.4.3 Show the updated values (1pt)"""

#print unique vals
print(df['last_new_job'].unique())

"""## 2.5 Preprocessing other columns (2pt total)

### 2.5.1 Drop the enrollee_id, any unnamed columns, and any duplicate columns (if you created multiple columns one with original and one with updated, then remove the original one) (2pt)
"""

# TODO (first col, enrolle_id, check for[city and city development, experince and relevent experince])
colDrop = ['Unnamed: 0','enrollee_id']

for d in colDrop:
  df.drop(d, axis = 1,inplace = True)
print(df)

# TODO
print(df)

"""## 2.6 Feature Scaling (3+1 = 4ps total)

### 2.6.1 Use sklearn.preprocessing's MinMaxScaler to perform min max scaling to all the numeric columns (3pt)
"""

# TODO
from sklearn.preprocessing import MinMaxScaler
numCol = ['education_level','city_development_index','experience','company_size','training_hours','target']
scale = MinMaxScaler()
print(scale.fit(df[numCol]))

# TODO
print(scale,'\n',scale.data_max_)

"""### 2.6.2 Show some of the scaled records. (1pt)"""

# TODO
print(scale,'\n',scale.data_max_,'\n',scale.data_min_)

"""# 3. X/Y and Training/Test Split with stratified sampling (15pts in total)

### 3.1 Using a lot of features with categorical values is not memory-efficient. Use a LabelEncoder() to convert all the categorical columns to numeric labels. (This task is similar to previous assignment A1) (2pt)
"""

from sklearn.preprocessing import LabelEncoder
# TODO check for education level
#gender, relevent exp, enrolledUni, major discipline,comp type,
catagorical =['city', 'gender','relevent_experience', 'enrolled_university','major_discipline','company_type']
new_df = df.copy()
for change in catagorical:
  new_df[change] = LabelEncoder().fit(new_df[change]).transform(new_df[change])
print(new_df)

"""### 3.2 Copy all the features into X and the target to Y (2pt)"""

# TODO
X = new_df.copy()
X.drop(['target'],axis = 1, inplace=True)
Y = new_df['target'].copy()

"""### 3.3 Show the ratio of 1 and 0 in Y. (1pt)"""

# TODO
print(Y.value_counts())
def ratio(df):
  sum1 = df[df.values != 0]
  sum0 = df[df.values != 1]
  oneR = sum1.count()/sum0.count()
  zeroR = sum0.count()/sum1.count()
  return oneR, zeroR
oneR,zeroR = ratio(Y)
print('ratio of 1 = ', oneR, '\nratio of 0 = ', zeroR)

"""### 3.4 Use sklearn's train_test_split() to split the data set into 70% training and 30% test sets. Set random_state to 42. We want to have the same ratio of 0 and 1 in the test set, use the stratify parameter to Y to ensure this. Then show the ratio of 1 and 0 in both train and test target. (4pt)"""

# TODO ask for method for ratio
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = .3, random_state = 42,stratify= Y)
Rzero,Rone= ratio(Y_test)
print('test ratios for 1 and 0 are: ', Rzero,', ',Rone)
Rzero,Rone= ratio(Y_train)
print('train ratios for 1 and 0 are: ', Rzero,', ',Rone)

"""### 3.5 Rebalancing (4+2 = 6pts)

3.5.1 Use imblearn's SMOTENC to balance the x_train

When our training set have class imbalance, we often perform over-sampling to generate synthetic data that can help in training. SMOTE is a library by imblearn for this purpose. The usage is fairly straightforward. See documentation [here](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html#imblearn.over_sampling.SMOTENC) and a brief explanation with example [here](https://medium.com/analytics-vidhya/smote-nc-in-ml-categorization-models-fo-imbalanced-datasets-8adbdcf08c25)
"""

print(X_train.head())
#print(Y_train.head())

# TODO catagorical feature issue
#, 'education_level','relevent_experience','enrolled_university', 'major_discipline', 'company_type']

from imblearn.over_sampling import SMOTENC
catCol = [0,2,3,4,6,8]
sm = SMOTENC(random_state=42, categorical_features = catCol)
rX_Train,rY_train = sm.fit_resample(X_train,Y_train)

"""3.5.2 Did that change the ratio in label? Confirm by printing the ratio in resampled labels."""

# TODO figure out how to pull labels from sm

"""# 4. Decision Tree (20pts total)

### 4.1 Initialize a decision tree model using sklearns DecisionTreeClassifier. Use the unbalanced training set. Set a consistent value for random_state parameter so that your result is reproducible. (1pt)
"""

# TODO
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(random_state = 42)
dt.fit(X_train, Y_train)

"""### 4.2 Use grid search to find out the best combination of values for the parameters: criterion, max_depth, min_samples_split, max_features. Then print the best performing parameters. (4pt)"""

# TODO DO we just select vals for params dictionary for function or
#params criterion, maxDepth, minSampleSplit,maxFeature
from sklearn.model_selection import GridSearchCV
params ={'criterion':['gini', 'entropy'],
         'max_depth': [1, 5, 10, 20, 50, 100],
         'min_samples_split': [2,5,10,20,50,100],
         'max_features': [None,'auto', 'sqrt', 'log2', 1, 2, 3,4,5,6]}
gs = GridSearchCV(estimator = dt, param_grid = params, error_score ='raise')
gs.fit(X_train,Y_train)

"""### 4.3 Add the best performing parameter set to the already-initialized Decision Tree model. Then fit it on the train dataset. (2pt)"""

# TODO
params = gs.best_params_
dt.set_params(criterion= 'entropy', max_depth = 5, max_features= 6, min_samples_split = 50)
dt.fit(X_train,Y_train)

"""### 4.4 Import the accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score from scikitlearn's metrics package. Evaluate your Decision Tree on the Test dataset and print all the metrics. (3pt)"""

# TODO wait for some previous steps then fin metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score

#get metrics from dt
Y_pred = dt.predict(X_test)
a_score = accuracy_score(Y_test,Y_pred)
p_score = precision_score(Y_test,Y_pred)
r_score = recall_score(Y_test,Y_pred)
conf_matrix = confusion_matrix(Y_test,Y_pred)
f_score = f1_score(Y_test,Y_pred)
ra_score = roc_auc_score(Y_test,Y_pred)

#print metrics
print("accuracy score = ", a_score)
print("Precision score = ",p_score)
print("Recall score = ", r_score)
print("Confusion matrix = ", conf_matrix)
print("f1 score = ", f_score)
print("roc auc score = ", ra_score)

"""### 4.5 Plot the tree using scikitlearn's tree package. You may need to define a large figure size using matplotlib to have an intelligible figure. (2pt)"""

# TODO waiting on steps above to complete, for large figsize is it done
# manual or from the system
#check doc for matplotlib
from sklearn import tree
width = dt.get_n_leaves()
height = dt.get_depth()
plt.figure(figsize = [60,30])
tree.plot_tree(dt, fontsize = 10)
plt.show()

"""### 4.6 Initialize a new Decision Tree model, then use the best set of parameters from Step 4.3 to train it on the balanced train set that you prepared in Step 3.5.1. (3pt)"""

# TODO
best_dt = DecisionTreeClassifier(criterion= 'entropy', max_depth = 5, max_features= 6, min_samples_split = 50)

"""### 4.7 Print the evaluation scores (accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score) from the training on balanced dataset. (3pt)"""

# TODO
#get metrics from best dt
best_dt.fit(X_train,Y_train)
Y_pred = best_dt.predict(X_test)
a_score = accuracy_score(Y_test,Y_pred)
p_score = precision_score(Y_test,Y_pred)
r_score = recall_score(Y_test,Y_pred)
conf_matrix = confusion_matrix(Y_test,Y_pred)
f_score = f1_score(Y_test,Y_pred)
ra_score = roc_auc_score(Y_test,Y_pred)

#print metrics
print("accuracy score = ", a_score)
print("Precision score = ",p_score)
print("Recall score = ", r_score)
print("Confusion matrix = ", conf_matrix)
print("f1 score = ", f_score)
print("roc auc score = ", ra_score)

"""### 4.8 Discuss any difference between evaluation results from the unbalanced train set and balanced train set. (2pt)

SO it seems to be that the balanced set seems to be better overall compared to the unbalanced set this is with the accuracy percision and recal along side the true positive in the confusion matrix however it does seem to just a little bit off by about 1 on the other data points.

# 5. Random Forest Classifier (12pts total)

### 5.1 Use grid search to find best combinations of the following Random Forest parameters: n_estimators, max_depth, min_samples_split and min_samples_leaf. Use your own choice of scoring, criterion, number of folds for cross-validation for the model initialization. Remember the grid search can take a while to finish. (4pt)
"""

# TODO
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state = 42)
rf.fit(X_train,Y_train)
params ={'n_estimators':[100,150,200], 'max_depth':[None, 5, 10 ,20], 'min_samples_split':[2,5,10,20], 'min_samples_leaf':[5,10,20,50,100]}
gs = GridSearchCV(estimator = rf, param_grid = params, error_score ='raise')
gs.fit(X_train,Y_train)

"""### 5.2 Print the best combination of parameters and use it to train a Random Forest classifier model. (3pt)"""

# TODO
bst_params = gs.best_params_
print(bst_params)
rf.set_params(max_depth = 10, min_samples_leaf = 50, min_samples_split = 2, n_estimators = 100)
rf.fit(X_train,Y_train)

"""### 5.3  Evaluate using the same metrics as before (accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score) (5pt)"""

# TODO
Y_pred = rf.predict(X_test)
a_score = accuracy_score(Y_test,Y_pred)
p_score = precision_score(Y_test,Y_pred)
r_score = recall_score(Y_test,Y_pred)
conf_matrix = confusion_matrix(Y_test,Y_pred)
f_score = f1_score(Y_test,Y_pred)
ra_score = roc_auc_score(Y_test,Y_pred)

#print metrics
print("accuracy score = ", a_score)
print("Precision score = ",p_score)
print("Recall score = ", r_score)
print("Confusion matrix = ", conf_matrix)
print("f1 score = ", f_score)
print("roc auc score = ", ra_score)

"""# 6. Boosting Classifier (20 pts total)

## 6.1 AdaBoost Classifier (10 pts total)

### 6.1.1 Perform a grid search for best values for parameters={n_estimators, learning_rate} of an AdaBoostClassifier and the given training set. (4pt)
"""

# TODO
from sklearn.ensemble import AdaBoostClassifier
abc = AdaBoostClassifier(random_state = 42)
abc.fit(X_train,Y_train)
params = {'n_estimators':[10,50,100], 'learning_rate':[0.1,0.5,1.5,2.0,2.5,5.0,10.0]}
gs = GridSearchCV(estimator = abc, param_grid = params, error_score ='raise')
gs.fit(X_train,Y_train)

"""### 6.1.2 Train an AdaboostClassifier using the best parameter set you found in step 6.1.1 (3pt)"""

# TODO
bst_params = gs.best_params_
print(bst_params)
abc.set_params(learning_rate = 0.1, n_estimators = 100)
abc.fit(X_train,Y_train)

"""### 6.1.3 Evaluate using the same metrics as before (accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score) (3pt)"""

# TODO
Y_pred = abc.predict(X_test)
a_score = accuracy_score(Y_test,Y_pred)
p_score = precision_score(Y_test,Y_pred)
r_score = recall_score(Y_test,Y_pred)
conf_matrix = confusion_matrix(Y_test,Y_pred)
f_score = f1_score(Y_test,Y_pred)
ra_score = roc_auc_score(Y_test,Y_pred)

#print metrics
print("accuracy score = ", a_score)
print("Precision score = ",p_score)
print("Recall score = ", r_score)
print("Confusion matrix = ", conf_matrix)
print("f1 score = ", f_score)
print("roc auc score = ", ra_score)

"""## 6.2 Gradient Boosting Classifier (10 pts total)

### 6.2.1 Perform a grid search for best values for parameters={n_estimators, max_depth, learning_rate} of a GradientBoostingClassifier and the given training set. (4pt)
"""

# TODO
from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier(random_state = 42)
gbc.fit(X_train,Y_train)
params = {'n_estimators':[150,100,200],'max_depth': [3, 5, 10 ,20], 'learning_rate':[0.5,1.5,2.0,2.5,5.0]}
gs = GridSearchCV(estimator = gbc, param_grid = params, error_score ='raise')
gs.fit(X_train,Y_train)

"""### 6.2.2 Train a GradientBoostingClassifier using the best parameter set you found in step 6.2.1 (3pt)"""

# TODO
bst_params = gs.best_params_
print(bst_params)
gbc.set_params(learning_rate = 0.5, max_depth = 3, n_estimators = 100)
gbc.fit(X_train,Y_train)

"""### 6.2.3 Evaluate using the same metrics as before (accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score) (3pt)"""

# TODO
Y_pred = gbc.predict(X_test)
a_score = accuracy_score(Y_test,Y_pred)
p_score = precision_score(Y_test,Y_pred)
r_score = recall_score(Y_test,Y_pred)
conf_matrix = confusion_matrix(Y_test,Y_pred)
f_score = f1_score(Y_test,Y_pred)
ra_score = roc_auc_score(Y_test,Y_pred)

#print metrics
print("accuracy score = ", a_score)
print("Precision score = ",p_score)
print("Recall score = ", r_score)
print("Confusion matrix = ", conf_matrix)
print("f1 score = ", f_score)
print("roc auc score = ", ra_score)

"""# 7. Summary Discussion (4 pts)

1)Which model yields the highest precision?

2)Which model yields the lowest recall?

3)Which model yields the higest True Positive (TP)?

4)Which model yields the best performance overall?

ANSWERS:
1) It seems that the balanced parameter Decision tree produced the highest persision. being 58%

2) It seems that the balanced parameter decision tree also produces the lowest recal being 50%

3)It looks like ada boosting has the highest True Positive of them having a recall score of 52.9%

4)I think the model that produces the best resaults overall would be the ada boosting beacuse it tends to have the better distribution of scores where it only lacks marginally in the true postive column of the confusion matrix.
"""
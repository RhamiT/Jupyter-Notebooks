# -*- coding: utf-8 -*-
"""Homework_02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sJx6-8Vr2e2YFTMO8dY3S9RRKF2DbQ-K

# Logistic Regression Homework

This is the 2nd assignment for CAP 4630 and we will implement logistic regression and apply it to two
different datasets. \
You will use **"Tasks"** and **"Hints"** to finish the work. **(Total 100 Points)** \
You are **not** allowed to use Machine Learning libaries such as Scikit-learn and Keras.

**Task Overview:**
- Logistic Regression

PID: 512376\
Name: Rhami Thrower\
Colaboration: N/A

## 1 - Logistic Regression ##
### 1.1 Packages

Import useful packages for scientific computing and data processing.

**Tasks:**
1. Import numpy and rename it to np.
2. Import pandas and rename it to pd.
3. Import the pyplot function in the libraray of matplotlib and rename it to plt.

References:
- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.
- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.

**Attention:**
1. After this renaming, you will use the new name to call functions. For example, **numpy** will become **np** in the following sections.
"""

# Import and rename libraries here
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""### 1.2 - Data Preparation ##

Prepare the data for regression task. **(20 Points)**

**Tasks:**
1. Load data for logistic regression.
2. **Generate the SCATTER PLOT of the data**.

**Hints:**
1. The data file is "data_logistic.csv", which are exam scores for students.
2. The data is organized by column: x1 (exam 1 score), x2 (exam 2 score), and label y (pass 1 or fail 0).
3. Please use different colors for postive(label=1) and negative(label=0) data.
4. An example of scatter plots is shown below.

![](https://drive.google.com/uc?export=view&id=1CPv5s4W8SkUMa_sXCIz-NejSnFj-e1IH)
"""

# Preprocess input data and generate plots
"""     uploading data     """
from google.colab import files
uploaded = files.upload()

"""       creating df      """
data = pd.read_csv("data_logistic.csv");
df = pd.DataFrame(data = data)
"""       creating plt     """
#creating df on pass and fail data
pass_data = df[df['label'] == 1].copy()
fail_data = df[df['label'] == 0].copy()
#plot data
x1 = pass_data['x1']
x2 = fail_data['x1']
y1 = pass_data['x2']
y2 = fail_data['x2']

plt.scatter(x1,y1, label = "Pass", marker = "+", color = "gold")
plt.scatter(x2,y2, label = "Fail", marker = "o", color = "purple")
plt.xlabel("X1")
plt.ylabel("x2")
plt.title("Pass v.s Fail Scores")
plt.legend()
plt.show()

"""### 1.3 - Sigmoid function ##


Implement sigmoid function so it can be called by the rest of your program. **(20 Points)**

**Tasks:**
1. Implement the sigmoid function (**def sigmoid(z):**).
2. Test the sigmoid function by function **plotting** with test data (X, Y) where Y = sigmoid(X).

**Hints:**  
1. Given the class material, sigmoid function is defined as:
$g(z) = \frac{1}{1+e^{-z}}$.
2. You may consider X = np.linspace(-5, 5, 1000) to plot the curve.
3. Plot Y against X.
4. An example of plot for validation is shown below:

![](https://drive.google.com/uc?export=view&id=18j5oHdw78uVm2WwHsdIb4hwhpXDxR37S)
"""

# Implement sigmoid fuction here
import math
def sigmoid(z):
  return (1/(1 + math.e**(-z)))

# Plot data here
X = np.linspace(-5, 5, 1000)
Y = sigmoid(X)

plt.plot(X,Y)
plt.show()

"""### 1.4 - Cost function and gradient ##

Implement the cross entropy cost function and its gradient for logistic regression. **(30 Points)**

**Tasks:**
1. Implement the "cal_cost" to compute the cost.
2. Implement the "cal_grad" to compute the gradients.
3. Test "cal_cost" and "cal_grad" with initial values and print out the results.

**Hint:**
1. The cross entropy cost function (J(θ)) in logistic regression is shown below. It involves two terms, including ylog(h) and (1-y)log(1-h) where h is the function of x.

![](https://drive.google.com/uc?export=view&id=1xLhlPFI4wekwuA7lFm7ebRVt0XBZk3e7)

2. The gradient of the cost J(θ) is a vector of the same length as θ where the $j$th element (for $j = 0, 1, . . . , n)$ is defined below. You may do a hand calculation to justify the first order derivative with the term above.

![](https://drive.google.com/uc?export=view&id=1xfA0A0xyRv2L5JZIdedAmEZxZ3DwpOCF)

3. When you implement J(θ), please use eps = 1e-15 to prevent possible "divide by 0 exception" in second term. You may think about the reason.
4. You may consider the below templates for two functions:

    def cal_cost(theta, X, y):

        htheta = ...
        term1 = ...  /* matrix_multiplication(log(htheta), y)
        term2 = ...  /* matrix_multiplication(log(1-htheta+eps), (1-y))
        J = - 1 / m * (term1 + term2)
        
        return cost
        
    
    def cal_grad(theta, X, y):
        
        htheta = ...
        term1 = ... /* matrix_multiplication(transpose(X), (htheta - y))  //you may think about why transpose(x)
        grad = 1 / m * term1
    
        return grad
5. It involves matrix multiplication and you may consider the function of np.matmul or np.dot.   
        
6. Initialize the intercept term (constant term) with **ones** and the theta with **zeros**. Test the functions with these initial values. \
    **Expected outputs:**\
    Cost at initial theta : 0.6931471805599445\
    Gradient at inital theta : [-0.1        -10.91242026 -11.73652937]

"""

# Implement the cost function here
"""
  htheta is

  theta is

  inputs are
    x is
    y is
    theta is
"""
def cal_cost(theta, X, y):
  bias = np.ones(len(df['x1']))
  bias = np.reshape(bias,(len(bias), 1))
  params = np.concatenate((bias, X), axis=1)
  htheta = sigmoid((np.dot(params,theta)))
  eps = 1e-15
  m = -len(df) # num elements being procesed
  term1 = np.matmul(np.log(htheta), y)
  term2 = np.matmul(np.log(1 - htheta + eps), (1 - y))
  cost = (1/m) * (term1 +term2)
  return cost

def cal_grad(theta, X, y):
  bias = np.ones(len(df['x1']))
  bias = np.reshape(bias,(len(bias), 1))
  params = np.concatenate((bias, X), axis=1)
  htheta = sigmoid((np.dot(params,theta)))
  m = len(df)
  term1 = np.matmul(np.transpose(params), (htheta - y))
  grad = (1 / m) * term1
  return grad

#test
#concatination for x1 and x2
#df[[‘x1’,’x2’]]
# y_hat = X1*W + x2*w + b
#            ^      ^   ^
#            0      0   0
#how to calculat matrix multiplication
bigX = np.array(df[['x1','x2']])
theta = np.zeros(3)
cost = cal_cost(theta,bigX, df['label'])
grad = cal_grad(theta,bigX, df['label'])
print("Cost at initial theta : ", cost)
print("\nGradient at inital theta : ", grad)

"""## 1.5 Train parameters with Gradient Descent ##


Train parameters using Gradient Descent. **(15 Points)**

**Tasks:**
1. Calculate best fit theta by Gradient Descent with learning rate of **0.001 (1e-3)** and epoch of **80K**. The initial theta from above blocks is used as initial values.
2. Print out the best theta (the last one is considered as the best here) and its corresponding cost.
3. **Plot the decision boundary**.

**Hints:**
1. You may take gradient descent in homework 1 as an template.
2. Derive the boundary line from **sigmoid(theta[0]+ X1 * theta[1] + X2* theta[2])=0.5**. Think about why we get the line by setting **the activated probability to 0.5**. Also, try to calculate the final relationship between X1 and X2. When sigmoid(X) = 0.5, what is the value of x? Check the generated plot in 1.3.
3. The validation of first 5 epochs (updated theta and cost): \
------Epoch 0------\
Theta: [0.0001     0.01091242 0.01173653]\
Cost: 0.6996118077359638\
------Epoch 1------\
Theta: [-0.0001129   0.00053949  0.00229352]\
Cost: 0.6649331468590681\
------Epoch 2------\
Theta: [-5.93604956e-05  8.33145873e-03  1.07754324e-02]\
Cost: 0.6679914364992459\
------Epoch 3------\
Theta: [-0.0002356   0.0004607   0.00370829]\
Cost: 0.6545873034874964\
------Epoch 4------\
Theta: [-0.00020363  0.00683227  0.01065138]\
Cost: 0.6563302142684528
4. You may take the plots below as an exmample:

![](https://drive.google.com/uc?export=view&id=1xLg9LrIF888gGXj3zRAG9iJLsyAmgPQg)

5. It may take ~1 min to finish running.
"""

# Gradient Descent Implementation Here
def regression():
  bigX = np.array(df[['x1','x2']])
  theta = [0, 0, 0]
  lr = 1e-3
  epochs = int(8e4)
  for n in range(epochs):
    y_pred = df['x1']*theta[1] +df['x2']*theta[2] + theta[0]
    #parital derivatives
    grad = cal_grad(theta, bigX, df['label'])
    theta[0] = theta[0] - lr * grad[0]
    theta[1] = theta[1] - lr * grad[1]
    theta[2] = theta[2] - lr * grad[2]
    # cost
    cost = cal_cost(theta, bigX, df['label'])
    if n <= 5:
      print('---------------------------\nepoch ', n, '\nTheta: ', theta)
      print('cost: ', cost)
  return theta,cost
theta,cost = regression()
print('---------------------------\nFinal: ', '\nTheta: ', theta)
print('cost: ', cost)

# Draw Decision Boundary Here
X1 = np.linspace(min(df['x1']), max(df['x1']), 80)
X2 = - (theta[0] / theta[2]) - (theta[1] / theta[2]) * X1
fig,axis = plt.subplots()
axis.scatter(x1, y1, color = 'gold', marker = '+')
axis.scatter(x2, y2, color = 'purple', marker = 'o')
axis.plot(X1,X2, label = 'boundry')
fig.show()

"""
### 1.6 Evaluating Logistic Regression

Evaluate the model with given data. **(15 Points)**

**Tasks:**
1. Calculate the training accuracy and **PRINT IT OUT**.
2. Evaluate the predicted probability of the learnt model with x1 = 56 and x2 = 32 and **PRINT IT OUT**.


**Hints:**  
1. Positive(prediction>0.5) and negative(prediction<=0.5).
2. The prediction results are based on acceptance probability. Given the two exam scores, we expected the model yields either high probability of "fail" or low probability of "pass".
3. Training accuracy should be around **85%**."""

# Evaluate the model
"""accuracy = correct / total"""
y_pred = df['x1']*theta[1] +df['x2']*theta[2] + theta[0]
y_pred[y_pred > 0.5] = 1
y_pred[y_pred <= 0.5] = 0
# 1 = pass   0 = fail
prediction = (y_pred == df['label'])
true_predictions = prediction[prediction == 1]
accuracy = len(true_predictions) / len(y_pred)
print("accuracy = " , (accuracy * 100),"%")
print("positive Probability = ",sigmoid(56*theta[1] + 32*theta[2] + theta[0]))
print("negative  Probability = ",(1 - sigmoid(56*theta[1] + 32*theta[2] + theta[0])))
# -*- coding: utf-8 -*-
"""Rhami_Thrower_A1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mXY5dmxYtMShpwMU9QxNIaWPuwShcPIT

# This jupyter notebook is prepared by “Rhami Thrower”.

### 1. Run the block below to upload the dataset. (Note that the file list gets refreshed every time your runtime is disconnected. Simply run this when you return to upload the file again using the files API. Once you run, it should wait for you to upload the file. (1pt)
"""

from google.colab import files
uploaded = files.upload()

"""### 2. Import numpy, pandas, matplotlib.pyplot and seaborn packages. (2pt)
###### If you need additional packages, you can import it on the go in any code-block below.
"""

import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns

"""### 3. Import the dataset into a pandas dataframe. Then report how many rows and columns are present in the dataset. (2pt)"""

start_up_df = pd.read_csv('startup_info_.csv')
num_col = len(start_up_df.columns)
num_row = len(start_up_df.index)
print('Num rows = ',num_row, '\nNum columns = ',num_col,"\n\n")

"""### 4. Call the describe method to see summary statistics of the numerical attribute columns. (1pt)"""

print(start_up_df.describe())

"""### 5.1 List all attribute columns (1pt)"""

col_attribute = start_up_df.columns
print(col_attribute, '\n')

"""## 5.2 The "Unnamed: 0","Unnamed: 6", "state_code.1" and "object_id" feature columns are not useful. Drop them in-place. (1pt)"""

start_up_df.drop(columns = ['Unnamed: 0' , 'Unnamed: 6', 'state_code.1', 'object_id'], inplace =True)
print(start_up_df)

"""## 6.1 Show all the numeric columns and save it to a new dataframe. (2pt)"""

startup_numerical_df = start_up_df.select_dtypes(include = ['double','int'])
print(startup_numerical_df)

"""## 6.2 Plot distributions of the numeric columns using histogram and record the skew of each distribution. (Note: positive value = right skewed, negative value = left skewed) (4pt)"""

#get a list of names of each column
"""does skew need to be on graph or presented else where"""
listNames =list(startup_numerical_df)
skew = np.empty(len(startup_numerical_df))
i=0

for col in listNames:
  max = startup_numerical_df[col].max();
  skew[i] = startup_numerical_df[col].skew()
  print('skew = ', skew[i])
  sns.set()
  sns.histplot(data = startup_numerical_df, x = col,  kde = True,bins = 15)
  plt.xlabel(col + " range")
  plt.ylabel(col + " count")
  plt.title(col)
  print(col +' skew is ', 'left' if skew[i] <= 0 else 'right')
  plt.show()
  plt.clf()

"""## 7. Show all the categorical columns and save it to a new dataframe. (2pt)"""

#is brute force ok
startup_categorical_df = start_up_df[['labels','is_top500','status']]
print(startup_categorical_df)

"""## 8. Examine missing values (2+2+3=7pt)

### 8.1 Show a list with column wise count of missing values and display the list in count wise descending order.
"""

i = 0
listNames =list(start_up_df)
numMissing = np.empty(len(listNames))
i = 0
#get missing vals
for col in listNames:
  numMissing[i] = start_up_df[col].isna().sum()
  i = i + 1
missingVals = pd.DataFrame()
missingVals['features'] = listNames
missingVals['num_miss'] = numMissing
missingVals = missingVals.sort_values(by = ['num_miss'] , ascending=False )
for val in missingVals['num_miss']:
  if val > 0:
    print(val)

"""### 8.2 Show columnwise percentage of missing values."""

i = 0
listNames =list(start_up_df)
percent_missing = np.empty(len(listNames))
for value in missingVals['num_miss']:
  percent_missing[i] = ((numMissing[i]/924) * 100)
  i = i + 1
percentMiss = pd.DataFrame()
percentMiss['features'] = listNames
percentMiss['p_miss'] = percent_missing
percentMiss = percentMiss.sort_values(by = ['p_miss'] , ascending=False )
for val in percentMiss['p_miss']:
  if val > 0:
    print(val)

"""### 8.3 Display a bar plot to visualize only the columns with missing values and their percentage count."""

delete = [0,13,22,21,20,19,18,17,16,12,1,11,10,8,7,6,5,4,3,2,23]
percentMiss = percentMiss.drop(index = delete)
sns.barplot(data = percentMiss, x = 'features', y = 'p_miss')
plt.xticks(rotation = 90)
plt.title('Percent of missing values per Feature')
plt.xlabel('Features')
plt.ylabel('percent missing')

"""## 9. Label Encoding : Copy the dataframe to a new one. Then using scikitlearn's Label Encoder, transform the "status" column to 0-1. (5pt)"""

from sklearn.preprocessing import LabelEncoder
new_su_df = start_up_df.copy()
new_su_df['status'] = LabelEncoder().fit(new_su_df['status']).transform(new_su_df['status'])
print(new_su_df)

"""## 10. Correlation: Use seaborn's heatmap to visualize the correlation between numeric features. (3pt)"""

#calc correlation between data and assign to proper column
names = list(startup_numerical_df)
revNames = np.flip(names)
correlationGrid = np.empty([len(names),len(names)])
correlationSet = np.empty(len(names))
for i in range(len(names)):
  for j in range(len(names)):
    correlationSet[j] = startup_numerical_df[names[i]].corr(startup_numerical_df[names[j]])
  correlationGrid[i] = correlationSet
sns.heatmap(correlationGrid,annot = True, xticklabels = names, yticklabels = names, annot_kws={'size': 5})

"""## 11.1 Use seaborn's countplot to visualize relationship between "*state_code*" and "*labels*". Comment on which state produced majority of successful startups (3pt)"""

visualStartup1 = sns.countplot(data = start_up_df, x= 'state_code', hue = 'labels')
visualStartup1.set_xticklabels(visualStartup1.get_xticklabels(), rotation = 90)
plt.show()

"""It looks like California (CA) produce the majority of successful startups.

## 11.2 Use seaborn's countplot to visualize relationship between "*milestones*" and "*labels*". Comment on which milestone made the statistically highest number of successful startups (3pt)
"""

visualStartup1 = sns.countplot(data = start_up_df, x= 'milestones', hue = 'labels')
visualStartup1.set_xticklabels(visualStartup1.get_xticklabels(), rotation = 90)
plt.show()

"""It looks like milestone 2 produced the most successful startups

## 12. Drop features with duplicate values in-place, then show dataframe's new shape. (1pt)
"""

bu_start_up_df = start_up_df.copy()#save old data frame just incease need later
name = list(start_up_df)
start_up_df.drop_duplicates(inplace= True)
print(start_up_df.shape)#checked with the remove colums in excell their was none

"""## 13. From correlation heatmap above, comment on which feature has the highest correlation with "*funding_rounds*". Visualize a scatterplot with that and "*funding_rounds*". (3+3 = 6pt)

either funding rounds or relationships have the highest correlation from the heat map above
"""

x = startup_numerical_df['funding_rounds']
y = startup_numerical_df['age_last_funding_year']

plt.plot(x,y,linestyle = "none", marker ='.')
plt.xlabel('Funding rounds')
plt.xlabel('last funding year')

"""## 14. Show boxplots for the numeric features to detect outliers. (4pt)"""

#normalize data
norm_df = (startup_numerical_df/startup_numerical_df.mean())/startup_numerical_df.std()
grid = sns.boxplot(data = norm_df,orient = 'v')
plt.setp(grid.get_xticklabels(), rotation=90)
plt.show()

"""## 15. Summary and Discussion: Mention what additional steps are required to use this dataset in a binary classifier. Eg: any column to remove, any record to remove, any distribution to rebalance, any features to be joined together to generate new feature etc. (2pt)

their are ways to optimize this data for a binary classifier. first I would remove the name, id, latitude, longitude. I would remove these primarly because they don't add any value to a binary classifier and would slow it down. Take for instanec two examples the name which doesn't contribute to a success of a startup and the longitude and latitude which could be replaced with state code and zip code to classify successful areas for a startup. as for combining data i would combine first and last funding years to compute a value and i would do the same for the milestone year as well. This is beacuse you can calculate a value from these coloumns to get a total of start and end which wouldn't result in to much data loss. for rebalincing featrues i would rebalence is top 500 and status since both seem to have a higher distribution of one value thus it could cause overfiting of data from that aspect.
"""